import streamlit as st
import whisper
import openai
import os
import tempfile
import mammoth
import docx2txt
import pandas as pd
import pptx
from PyPDF2 import PdfReader
from PIL import Image
import pytesseract

# åˆæœŸåŒ–
if 'summary' not in st.session_state:
    st.session_state.summary = ""
if 'suggestion' not in st.session_state:
    st.session_state.suggestion = ""

openai.api_key = st.secrets["openai_key"]

st.title("GPT Meeting Assistant")

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
prompt_input = st.text_area("ğŸ”§ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆä»»æ„ï¼‰", placeholder="ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã€‚æœªå…¥åŠ›ãªã‚‰ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼è¦–ç‚¹ãŒä½¿ã‚ã‚Œã¾ã™ã€‚")
agenda_files = st.file_uploader("ğŸ“‚ ã‚¢ã‚¸ã‚§ãƒ³ãƒ€ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæœ€å¤§10å€‹ï¼‰", type=["pdf", "doc", "docx", "xls", "xlsx", "ppt", "pptx", "txt", "jpg", "jpeg", "png", "gif"], accept_multiple_files=True)
audio_file = st.file_uploader("ğŸ§ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["mp3", "mp4", "wav", "m4a"])

# åˆ†æé–‹å§‹
if st.button("åˆ†æã‚’é–‹å§‹"):
    if audio_file is not None:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as temp_audio:
            temp_audio.write(audio_file.read())
            temp_audio_path = temp_audio.name

        st.write("ğŸ” Whisperã§æ–‡å­—èµ·ã“ã—ä¸­...")
        transcription = whisper.load_model("medium").transcribe(temp_audio_path, language="ja")
        text = transcription["text"]

        st.write("ğŸ“‘ ã‚¢ã‚¸ã‚§ãƒ³ãƒ€èª­ã¿è¾¼ã¿ä¸­...")
        agenda_texts = []
        for f in agenda_files:
            ext = f.name.split(".")[-1].lower()
            with tempfile.NamedTemporaryFile(delete=False, suffix="." + ext) as tmp:
                tmp.write(f.read())
                path = tmp.name

            try:
                if ext in ["txt"]:
                    with open(path, "r", encoding="utf-8") as file:
                        agenda_texts.append(file.read())
                elif ext in ["pdf"]:
                    pdf = PdfReader(path)
                    content = "\n".join([p.extract_text() or "" for p in pdf.pages])
                    agenda_texts.append(content)
                elif ext in ["docx"]:
                    agenda_texts.append(docx2txt.process(path))
                elif ext in ["doc"]:
                    result = mammoth.convert_to_markdown(open(path, "rb"))
                    agenda_texts.append(result.value)
                elif ext in ["xls", "xlsx"]:
                    df = pd.read_excel(path)
                    agenda_texts.append(df.to_string(index=False))
                elif ext in ["ppt", "pptx"]:
                    pres = pptx.Presentation(path)
                    slides_text = []
                    for slide in pres.slides:
                        for shape in slide.shapes:
                            if hasattr(shape, "text"):
                                slides_text.append(shape.text)
                    agenda_texts.append("\n".join(slides_text))
                elif ext in ["jpg", "jpeg", "png", "gif"]:
                    img = Image.open(path)
                    text_img = pytesseract.image_to_string(img, lang="jpn")
                    agenda_texts.append(text_img)
            except Exception as e:
                agenda_texts.append(f"(èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {f.name})")

        agenda = "\n\n".join(agenda_texts)
        role_prompt = prompt_input.strip() or (
            "ã‚ãªãŸã¯ä¿¡é ¼ã§ãã‚‹ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã¨ã—ã¦ã€ä¼šè©±ã®æµã‚Œã‚’å¤§åˆ‡ã«ã—ãªãŒã‚‰ã€"
            "ç´ æ™´ã‚‰ã—ã„è¦–ç‚¹ã«ã¯å…±æ„Ÿã‚’ç¤ºã—ã€è­°è«–ã«è¶³ã‚Šãªã„è¦–ç‚¹ã«ã¯èª°ã‚‚ãŒãƒãƒƒã¨ã™ã‚‹ã‚ˆã†ãªå•ã„ã‚’ãƒ¦ãƒ¼ãƒ¢ã‚¢ã‚’äº¤ãˆã¦æç¤ºã§ãã¾ã™ã€‚"
            "å•ã„ã‚’å‡ºã™ã¨ãã¯ã€ãªãœãã®å•ã„ãŒå¿…è¦ãªã®ã‹ã€ç­”ãˆãªã‘ã‚Œã°èµ·ã“ã‚Šå¾—ã‚‹ãƒªã‚¹ã‚¯ã‚„æœªæ¥ã®ã‚ºãƒ¬ã‚’å…·ä½“ä¾‹ã§ç¤ºã—ã¦ãã ã•ã„ã€‚"
            "å•ã„ã®ãƒˆãƒ¼ãƒ³ã¯æŸ”ã‚‰ã‹ãã€ãã‚Œã§ã„ã¦é‹­ãã€‚ã€Œç¢ºã‹ã«â€¦ãã‚Œã€å¤§äº‹ã§ã™ã­ã€ã¨æ€ã‚ã›ã‚‹å•ã„ã‚’ç›®æŒ‡ã—ã¦ãã ã•ã„ã€‚"
        )

        # è¦ç´„
        st.write("ğŸ§  ChatGPTã§è¦ç´„ä¸­...")
        summary_prompt = f"""
ä»¥ä¸‹ã¯ä¼šè­°ã®æ–‡å­—èµ·ã“ã—ã§ã™ã€‚ä»¥ä¸‹ã®4ç‚¹ã‚’éµå®ˆã—ã€äº‹å®Ÿã®ã¿ã‚’ãƒ“ã‚¸ãƒã‚¹å‘ã‘ã®ä¸å¯§ãªæ–‡ç« ã§ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚
1.è­°è«–ã®ãƒã‚¤ãƒ³ãƒˆã‚’æ¼ã‚Œãªã
2.èª­ã¿ã‚„ã™ã
3.ç°¡æ½”ã«
4.æ§‹é€ çš„ã«

ã€Šã‚¢ã‚¸ã‚§ãƒ³ãƒ€ã€‹
{agenda}

ã€Šæ–‡å­—èµ·ã“ã—ã€‹
{text}
"""
        summary_response = openai.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": summary_prompt}]
        )
        st.session_state.summary = summary_response.choices[0].message.content

        # ææ¡ˆ
        st.write("ğŸ’¡ ChatGPTã§ææ¡ˆä¸­...")
        suggestion_prompt = f"""
ä»¥ä¸‹ã¯ä¼šè­°ã®æ–‡å­—èµ·ã“ã—ã§ã™ã€‚

ã€Šã‚¢ã‚¸ã‚§ãƒ³ãƒ€ã€‹
{agenda}

ã€Šæ–‡å­—èµ·ã“ã—ã€‹
{text}

ã“ã®å†…å®¹ã‚’ã‚‚ã¨ã«ã€å‚åŠ è€…ã®æ°—ã¥ãã‚’ä¿ƒã™ã‚ˆã†ãª
- æ”¹å–„ç‚¹
- å•ã„
- ãƒªã‚¹ã‚¯ã¨ãã®å›é¿ç­–

ã‚’å…·ä½“ä¾‹ã‚„ç†ç”±ã‚’æ·»ãˆã¦ã€æ·±ã„å…±æ„ŸãŒå¾—ã‚‰ã‚Œã‚‹å½¢ã§ææ¡ˆã—ã¦ãã ã•ã„ã€‚
"""
        suggestion_response = openai.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": role_prompt},
                {"role": "user", "content": suggestion_prompt}
            ]
        )
        st.session_state.suggestion = suggestion_response.choices[0].message.content

        st.success("âœ… åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼ä¸‹ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚")

# å‡ºåŠ›è¡¨ç¤ºï¼†ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå¸¸æ™‚è¡¨ç¤ºï¼‰
if st.session_state.summary:
    st.subheader("ğŸ“„ ä¼šè­°è¦ç´„")
    st.text_area("ä¼šè­°è¦ç´„ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", st.session_state.summary, height=200)
    st.download_button(
        label="â¬‡ è¦ç´„ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=st.session_state.summary,
        file_name="summary.txt",
        mime="text/plain",
        key="download_summary"
    )

if st.session_state.suggestion:
    st.subheader("ğŸ’¡ ChatGPTã®ææ¡ˆ")
    st.text_area("ææ¡ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", st.session_state.suggestion, height=200)
    st.download_button(
        label="â¬‡ ææ¡ˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=st.session_state.suggestion,
        file_name="suggestions.txt",
        mime="text/plain",
        key="download_suggestion"
    )
